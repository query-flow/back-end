"""
Service for generating query suggestions to help lost users
"""
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from app.repositories.query_history_repository import QueryHistoryRepository
from app.schemas.suggestion_schema import QuestionSuggestion, SuggestionSource
from app.pipeline.llm.client import call_llm

logger = logging.getLogger(__name__)


class SuggestionService:
    """
    Generates intelligent query suggestions through 3 layers:

    1. Static: Pre-configured questions from JSON (fast, always available)
    2. Personalized: Based on user's query history (user-specific)
    3. Contextual: Generated by LLM based on current result (smart follow-ups)
    """

    def __init__(self, query_history_repo: QueryHistoryRepository):
        self.query_history_repo = query_history_repo
        self._load_static_questions()

    def _load_static_questions(self):
        """Load static questions from JSON file"""
        config_path = Path(__file__).parent.parent / "config" / "suggested_questions.json"

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.static_questions = json.load(f)
            logger.info(f"Loaded static questions for {len(self.static_questions)} schemas")
        except Exception as e:
            logger.error(f"Failed to load static questions: {e}")
            self.static_questions = {}

    def get_static_suggestions(self, schema: str) -> List[str]:
        """
        Get pre-configured questions for a schema

        Layer 1: Static suggestions (instant, no DB/LLM calls)

        Args:
            schema: Schema name (e.g., "sakila", "world")

        Returns:
            List of pre-configured questions
        """
        # Try exact schema match
        if schema in self.static_questions:
            return self.static_questions[schema]["questions"]

        # Fallback to default questions
        if "_default" in self.static_questions:
            return self.static_questions["_default"]["questions"]

        return []

    def get_personalized_suggestions(
        self,
        user_id: str,
        limit: int = 5
    ) -> List[QuestionSuggestion]:
        """
        Get suggestions based on user's query history

        Layer 2: Personalized suggestions (DB query, fast)

        Args:
            user_id: User ID
            limit: Max suggestions to return

        Returns:
            List of QuestionSuggestion with frequency metadata
        """
        try:
            popular = self.query_history_repo.get_user_popular_questions(
                user_id=user_id,
                days=30,
                limit=limit
            )

            return [
                QuestionSuggestion(
                    question=item["pergunta"],
                    source=SuggestionSource(
                        type="personalized",
                        reason=f"Você perguntou isso {item['count']} vezes"
                    ),
                    metadata={
                        "frequency": item["count"],
                        "last_asked": item["last_asked"].isoformat()
                    }
                )
                for item in popular
            ]
        except Exception as e:
            logger.warning(f"Failed to get personalized suggestions: {e}")
            return []

    def get_org_popular_suggestions(
        self,
        org_id: str,
        schema: Optional[str] = None,
        limit: int = 5
    ) -> List[QuestionSuggestion]:
        """
        Get suggestions based on organization's popular queries

        Shows what other users in the org are asking

        Args:
            org_id: Organization ID
            schema: Optional filter by schema
            limit: Max suggestions to return

        Returns:
            List of QuestionSuggestion with popularity metadata
        """
        try:
            popular = self.query_history_repo.get_org_popular_questions(
                org_id=org_id,
                schema=schema,
                days=30,
                limit=limit
            )

            return [
                QuestionSuggestion(
                    question=item["pergunta"],
                    source=SuggestionSource(
                        type="popular",
                        reason=f"Pergunta popular ({item['user_count']} usuários)"
                    ),
                    metadata={
                        "total_count": item["count"],
                        "user_count": item["user_count"]
                    }
                )
                for item in popular
            ]
        except Exception as e:
            logger.warning(f"Failed to get org popular suggestions: {e}")
            return []

    def generate_contextual_suggestions(
        self,
        pergunta: str,
        colunas: List[str],
        dados: List[List[Any]],
        schema_used: str
    ) -> List[str]:
        """
        Generate smart follow-up questions using LLM

        Layer 3: Contextual suggestions (LLM call, slower but intelligent)

        Args:
            pergunta: Original question
            colunas: Result columns
            dados: Result data (sample)
            schema_used: Schema that was queried

        Returns:
            List of 3-4 suggested follow-up questions
        """
        if not colunas or not dados:
            return []

        try:
            prompt = self._build_contextual_prompt(
                pergunta, colunas, dados[:5], schema_used
            )

            response = call_llm(prompt, temperature=0.2, max_tokens=300)

            # Parse response (expecting JSON array)
            import json
            suggestions = json.loads(response)

            if isinstance(suggestions, list):
                return suggestions[:4]  # Max 4 suggestions

            return []

        except Exception as e:
            logger.warning(f"Failed to generate contextual suggestions: {e}")
            return []

    def _build_contextual_prompt(
        self,
        pergunta: str,
        colunas: List[str],
        dados_sample: List[List[Any]],
        schema_used: str
    ) -> List[Dict[str, str]]:
        """
        Build prompt for LLM to generate follow-up questions
        """
        system = """Você é um assistente de análise de dados.

Dado o resultado de uma query, sugira 3-4 perguntas de follow-up naturais e úteis.

TIPOS DE FOLLOW-UP:
1. **Drill-down**: Investigar item específico ("Mostre detalhes de [item]")
2. **Comparação**: Comparar com período/categoria ("Compare com mês anterior")
3. **Tendência**: Ver evolução temporal ("Como isso mudou ao longo do tempo?")
4. **Segmentação**: Ver por outra dimensão ("Mostre isso por região")
5. **Agregação**: Ver totais/médias ("Qual o total/média?")

REGRAS:
- Perguntas devem ser NATURAIS (como um humano perguntaria)
- Devem ser RELEVANTES aos dados mostrados
- Devem ser EXECUTÁVEIS (possível gerar SQL)
- Máximo 10 palavras por pergunta

FORMATO DE RESPOSTA (JSON array):
["Pergunta 1", "Pergunta 2", "Pergunta 3"]

EXEMPLO:
Pergunta original: "Mostre vendas por produto"
Resultado: [["Produto A", 1000], ["Produto B", 800]]
Sugestões: [
  "Qual a margem de lucro de cada produto?",
  "Compare com o trimestre anterior",
  "Mostre a tendência de vendas",
  "Quais clientes compraram mais?"
]"""

        user = f"""PERGUNTA ORIGINAL: {pergunta}

SCHEMA USADO: {schema_used}

RESULTADO (primeiras 5 linhas):
Colunas: {colunas}
Dados: {dados_sample}

Gere 3-4 perguntas de follow-up em JSON:"""

        return [
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]

    def detect_user_stuck(
        self,
        user_id: str,
        time_since_last_query_seconds: int
    ) -> bool:
        """
        Detect if user might be stuck/lost

        Simple heuristic: user inactive for 30+ seconds after seeing result

        Args:
            user_id: User ID
            time_since_last_query_seconds: Seconds since last query

        Returns:
            True if user appears stuck
        """
        # User idle for 30+ seconds = possibly stuck
        if time_since_last_query_seconds >= 30:
            return True

        # Could add more sophisticated detection:
        # - Failed queries
        # - Repeated similar questions
        # - No drill-downs after initial query

        return False
